"""
v0.12 é›†æˆæµ‹è¯•

éªŒè¯æ€§èƒ½æµ‹è¯•æ¡†æ¶ã€æ€§èƒ½åŸºå‡†å’ŒåŠŸèƒ½æ­£ç¡®æ€§
"""

import pytest
from pathlib import Path
import sys

# æ·»åŠ  mcda_core åˆ°è·¯å¾„
mcda_core_path = Path(__file__).parent.parent.parent / "skills" / "mcda-core" / "lib"
if str(mcda_core_path) not in sys.path:
    sys.path.insert(0, str(mcda_core_path))

from mcda_core.core import MCDAOrchestrator
from performance.utils import measure_execution_time, PerformanceCriteria, generate_performance_report


class TestV012EndToEnd:
    """v0.12 ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•"""

    def test_performance_framework_e2e(self, tmp_path):
        """æµ‹è¯•æ€§èƒ½æ¡†æ¶ç«¯åˆ°ç«¯æµç¨‹"""
        print("\nğŸš€ æµ‹è¯•æ€§èƒ½æ¡†æ¶ç«¯åˆ°ç«¯æµç¨‹...")

        orchestrator = MCDAOrchestrator()
        fixtures_dir = Path(__file__).parent.parent / "performance" / "fixtures"

        # æµ‹è¯•æ‰€æœ‰è§„æ¨¡
        test_results = {}
        test_configs = [
            ('small', 'small_10x5.yaml', 10, 5, 'small'),
            ('medium', 'medium_50x20.yaml', 50, 20, 'medium'),
            ('large', 'large_100x50.yaml', 100, 50, 'large'),
        ]

        for category, filename, alts, crits, perf_cat in test_configs:
            config_path = fixtures_dir / filename
            print(f"\nğŸ“Š æµ‹è¯• {category}: {alts}æ–¹æ¡ˆ Ã— {crits}å‡†åˆ™")

            # è¿è¡Œæ€§èƒ½æµ‹è¯•
            result = measure_execution_time(
                orchestrator.run_workflow,
                str(config_path)
            )

            # éªŒè¯æ€§èƒ½æ ‡å‡†
            criteria = PerformanceCriteria.check_performance(
                perf_cat, alts, crits,
                result['execution_time'],
                result['memory_mb']
            )

            # æ”¶é›†ç»“æœ
            test_results[category] = {
                'alternatives': alts,
                'criteria': crits,
                'execution_time': result['execution_time'],
                'memory_mb': result['memory_mb'],
                'response_time_ok': criteria['response_time_ok'],
                'memory_ok': criteria['memory_ok']
            }

            # éªŒè¯æ€§èƒ½è¾¾æ ‡
            assert criteria['response_time_ok'], \
                f"{category} å“åº”æ—¶é—´ {result['execution_time']:.3f}s è¶…è¿‡é™åˆ¶"
            assert criteria['memory_ok'], \
                f"{category} å†…å­˜ä½¿ç”¨ {result['memory_mb']:.1f}MB è¶…è¿‡é™åˆ¶"

            # éªŒè¯ç»“æœæœ‰æ•ˆæ€§
            assert result['result'] is not None
            assert hasattr(result['result'], 'rankings')
            assert len(result['result'].rankings) > 0

            print(f"   âœ… å“åº”æ—¶é—´: {result['execution_time']:.3f}s")
            print(f"   âœ… å†…å­˜ä½¿ç”¨: {result['memory_mb']:.1f}MB")
            print(f"   âœ… æ’åæ•°: {len(result['result'].rankings)}")

        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        output_file = tmp_path / "integration_performance_report.md"
        generate_performance_report(test_results, output_file)

        # éªŒè¯æŠ¥å‘Šç”Ÿæˆ
        assert output_file.exists()
        content = output_file.read_text(encoding='utf-8')
        assert "æ€§èƒ½æµ‹è¯•æŠ¥å‘Š" in content
        assert "10æ–¹æ¡ˆ Ã— 5å‡†åˆ™" in content
        assert "50æ–¹æ¡ˆ Ã— 20å‡†åˆ™" in content
        assert "100æ–¹æ¡ˆ Ã— 50å‡†åˆ™" in content

        print(f"\nâœ… æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ: {output_file}")

    def test_performance_baseline_consistency(self):
        """æµ‹è¯•æ€§èƒ½åŸºå‡†ä¸€è‡´æ€§"""
        print("\nğŸ” æµ‹è¯•æ€§èƒ½åŸºå‡†ä¸€è‡´æ€§...")

        orchestrator = MCDAOrchestrator()
        config_path = Path(__file__).parent.parent / "performance" / "fixtures" / "medium_50x20.yaml"

        # è¿è¡Œ 3 æ¬¡ï¼Œæ£€æŸ¥ä¸€è‡´æ€§
        execution_times = []
        for i in range(3):
            result = measure_execution_time(
                orchestrator.run_workflow,
                str(config_path)
            )
            execution_times.append(result['execution_time'])

        # è®¡ç®—æ ‡å‡†å·®
        import statistics
        mean_time = statistics.mean(execution_times)
        stdev_time = statistics.stdev(execution_times)
        cv = (stdev_time / mean_time) * 100  # å˜å¼‚ç³»æ•°

        print(f"   å¹³å‡å“åº”æ—¶é—´: {mean_time:.3f}s")
        print(f"   æ ‡å‡†å·®: {stdev_time:.4f}s")
        print(f"   å˜å¼‚ç³»æ•°: {cv:.2f}%")

        # éªŒè¯æ€§èƒ½ç¨³å®šæ€§ï¼ˆCV < 10%ï¼‰
        assert cv < 10, f"æ€§èƒ½ä¸ç¨³å®šï¼Œå˜å¼‚ç³»æ•° {cv:.2f}% > 10%"

        print("   âœ… æ€§èƒ½ç¨³å®š")


class TestV012Correctness:
    """v0.12 åŠŸèƒ½æ­£ç¡®æ€§æµ‹è¯•"""

    def test_all_algorithms_work_on_large_scale(self):
        """æµ‹è¯•æ‰€æœ‰ç®—æ³•åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šçš„æ­£ç¡®æ€§"""
        print("\nğŸ§ª æµ‹è¯•æ‰€æœ‰ç®—æ³•åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šçš„æ­£ç¡®æ€§...")

        orchestrator = MCDAOrchestrator()
        config_path = Path(__file__).parent.parent / "performance" / "fixtures" / "small_10x5.yaml"

        algorithms = ['topsis', 'vikor', 'wsm']

        for algo in algorithms:
            print(f"\n   æµ‹è¯• {algo.upper()} ç®—æ³•...")

            # ä¿®æ”¹é…ç½®ä½¿ç”¨ä¸åŒç®—æ³•
            import yaml
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            config['algorithm']['name'] = algo

            # ä¿å­˜ä¸´æ—¶é…ç½®
            import tempfile
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False, encoding='utf-8') as f:
                yaml.dump(config, f, allow_unicode=True)
                temp_config = f.name

            try:
                # è¿è¡Œç®—æ³•
                result = orchestrator.run_workflow(temp_config)

                # éªŒè¯ç»“æœ
                assert result is not None
                assert hasattr(result, 'rankings')
                assert len(result.rankings) > 0
                assert hasattr(result, 'metadata')
                assert result.metadata.algorithm_name == algo

                print(f"      âœ… {algo.upper()} è¿è¡ŒæˆåŠŸ")
                print(f"      âœ… ç”Ÿæˆ {len(result.rankings)} ä¸ªæ’å")
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                Path(temp_config).unlink()

    def test_ranking_consistency(self):
        """æµ‹è¯•æ’åä¸€è‡´æ€§"""
        print("\nğŸ” æµ‹è¯•æ’åä¸€è‡´æ€§...")

        orchestrator = MCDAOrchestrator()
        config_path = Path(__file__).parent.parent / "performance" / "fixtures" / "small_10x5.yaml"

        # è¿è¡Œ 2 æ¬¡ï¼Œæ£€æŸ¥æ’åä¸€è‡´æ€§
        rankings_list = []
        for i in range(2):
            result = orchestrator.run_workflow(str(config_path))
            # æå–æ’åï¼ˆæ–¹æ¡ˆåï¼‰
            ranking = [r.alternative for r in result.rankings]
            rankings_list.append(ranking)

        # éªŒè¯æ’åä¸€è‡´
        assert rankings_list[0] == rankings_list[1], "æ’åä¸ä¸€è‡´"

        print(f"   âœ… æ’åä¸€è‡´")
        print(f"   âœ… å‰3å: {rankings_list[0][:3]}")


class TestV012Documentation:
    """v0.12 æ–‡æ¡£å®Œæ•´æ€§æµ‹è¯•"""

    def test_performance_baseline_exists(self):
        """æµ‹è¯•æ€§èƒ½åŸºå‡†æŠ¥å‘Šå­˜åœ¨"""
        baseline_file = Path("docs/active/mcda-core/v0.12/v0.12-performance-baseline.md")
        assert baseline_file.exists(), "æ€§èƒ½åŸºå‡†æŠ¥å‘Šä¸å­˜åœ¨"

        content = baseline_file.read_text(encoding='utf-8')
        assert "æ€§èƒ½æµ‹è¯•æŠ¥å‘Š" in content
        assert "10æ–¹æ¡ˆ Ã— 5å‡†åˆ™" in content
        assert "50æ–¹æ¡ˆ Ã— 20å‡†åˆ™" in content

        print("   âœ… æ€§èƒ½åŸºå‡†æŠ¥å‘Šå®Œæ•´")

    def test_bottleneck_analysis_exists(self):
        """æµ‹è¯•ç“¶é¢ˆåˆ†ææŠ¥å‘Šå­˜åœ¨"""
        analysis_file = Path("docs/active/mcda-core/v0.12/bottleneck-analysis.md")
        assert analysis_file.exists(), "ç“¶é¢ˆåˆ†ææŠ¥å‘Šä¸å­˜åœ¨"

        content = analysis_file.read_text(encoding='utf-8')
        assert "æ€§èƒ½ç“¶é¢ˆåˆ†æ" in content
        assert "Top 5" in content

        print("   âœ… ç“¶é¢ˆåˆ†ææŠ¥å‘Šå®Œæ•´")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
